{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4849320,"sourceType":"datasetVersion","datasetId":2807884}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport cv2\nfrom tqdm import tqdm\n\n# Base directory for the dataset\nBASE_DIR = \"/kaggle/input/ucf101-action-recognition/\"\n\n# Parameters\nSEQ_LENGTH = 20  # Total sequence length\nINPUT_LENGTH = 10  # Input frames for the model\nIMG_WIDTH = 64\nIMG_HEIGHT = 64\nIMG_CHANNEL = 1\nBATCH_SIZE = 16\nEPOCHS = 50\nLR = 0.001\n\n# Load video data with proper handling of train/val/test split directories\ndef load_videos_from_split_directory(base_dir, split, seq_length, img_width, img_height):\n    \"\"\"\n    Load video frames from the specified split directory (train/val/test).\n    \"\"\"\n    split_dir = os.path.join(base_dir, split)\n    videos = []\n    categories = os.listdir(split_dir)\n    for category in tqdm(categories, desc=f\"Processing {split} categories\"):\n        category_path = os.path.join(split_dir, category)\n        if not os.path.isdir(category_path):  # Skip non-directory files\n            continue\n        \n        video_files = [f for f in os.listdir(category_path) if f.endswith(\".avi\")]\n        for video_name in video_files:\n            video_path = os.path.join(category_path, video_name)\n            cap = cv2.VideoCapture(video_path)\n            frames = []\n            while len(frames) < seq_length:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                frame = cv2.resize(frame, (img_width, img_height))\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n                frames.append(frame)\n            cap.release()\n            if len(frames) == seq_length:  # Only keep videos with enough frames\n                videos.append(np.array(frames, dtype=np.float32))\n    return np.array(videos)\n\n# Load datasets\nprint(\"Loading training data...\")\ntrain_data = load_videos_from_split_directory(BASE_DIR, \"train\", SEQ_LENGTH, IMG_WIDTH, IMG_HEIGHT)\ntrain_data = train_data / 255.0  # Normalize pixel values\ntrain_data = train_data.reshape((-1, SEQ_LENGTH, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL))\nprint(f\"Training data shape: {train_data.shape}\")\n\nprint(\"Loading validation data...\")\nval_data = load_videos_from_split_directory(BASE_DIR, \"val\", SEQ_LENGTH, IMG_WIDTH, IMG_HEIGHT)\nval_data = val_data / 255.0\nval_data = val_data.reshape((-1, SEQ_LENGTH, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL))\nprint(f\"Validation data shape: {val_data.shape}\")\n\nprint(\"Loading test data...\")\ntest_data = load_videos_from_split_directory(BASE_DIR, \"test\", SEQ_LENGTH, IMG_WIDTH, IMG_HEIGHT)\ntest_data = test_data / 255.0\ntest_data = test_data.reshape((-1, SEQ_LENGTH, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL))\nprint(f\"Test data shape: {test_data.shape}\")\n\n# Prepare input-output pairs for training and validation\nx_train, y_train = train_data[:, :INPUT_LENGTH], train_data[:, INPUT_LENGTH:]\nx_val, y_val = val_data[:, :INPUT_LENGTH], val_data[:, INPUT_LENGTH:]\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:43:13.064039Z","iopub.execute_input":"2024-12-02T14:43:13.064618Z","iopub.status.idle":"2024-12-02T14:43:30.909056Z","shell.execute_reply.started":"2024-12-02T14:43:13.064586Z","shell.execute_reply":"2024-12-02T14:43:30.907756Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\n","output_type":"stream"},{"name":"stderr","text":"Processing train categories:  25%|██▍       | 25/101 [00:17<00:52,  1.44it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_videos_from_split_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEQ_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_WIDTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_HEIGHT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize pixel values\u001b[39;00m\n\u001b[1;32m     55\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, SEQ_LENGTH, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL))\n","Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mload_videos_from_split_directory\u001b[0;34m(base_dir, split, seq_length, img_width, img_height)\u001b[0m\n\u001b[1;32m     38\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m<\u001b[39m seq_length:\n\u001b[0;32m---> 40\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model\n\n# Custom pixel-level accuracy metric\ndef pixel_accuracy(y_true, y_pred, threshold=0.5):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    correct_pixels = tf.abs(y_true - y_pred) < threshold\n    accuracy = tf.reduce_mean(tf.cast(correct_pixels, tf.float32))\n    return accuracy\n\n# PredRNN Cell Class\nclass PredRNNCell(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size, stride=1):\n        super(PredRNNCell, self).__init__()\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.conv_x = layers.Conv2D(\n            self.filters, self.kernel_size, strides=self.stride, padding=\"same\", activation=\"relu\"\n        )\n        self.conv_h = layers.Conv2D(\n            self.filters, self.kernel_size, strides=self.stride, padding=\"same\", activation=\"relu\"\n        )\n\n    def build(self, input_shape):\n        self.conv_x.build(input_shape)\n        self.conv_h.build((None, input_shape[1], input_shape[2], self.filters))\n        super(PredRNNCell, self).build(input_shape)\n\n    def call(self, inputs, states):\n        x, h = inputs, states\n        if h is None:\n            batch_size, height, width, _ = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], self.filters\n            h = tf.zeros((batch_size, height, width, self.filters), dtype=tf.float32)\n        xh = self.conv_x(x)\n        hh = self.conv_h(h)\n        h_next = tf.nn.relu(xh + hh)\n        return h_next, h_next\n\n# PredRNN Model Class\nclass PredRNN(tf.keras.Model):\n    def __init__(self, input_shape, hidden_size, output_channels, num_layers=3):\n        super(PredRNN, self).__init__()\n        self.layers_list = [PredRNNCell(hidden_size, (3, 3)) for _ in range(num_layers)]\n        self.conv_output = layers.Conv2D(output_channels, (3, 3), padding=\"same\")\n\n    def build(self, input_shape):\n        time_steps, height, width, channels = input_shape[1:]\n        for layer in self.layers_list:\n            layer.build((None, height, width, channels))\n            channels = layer.filters  # Update channels for the next layer\n        self.conv_output.build((None, height, width, channels))\n        super(PredRNN, self).build(input_shape)\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        time_steps, height, width = inputs.shape[1], inputs.shape[2], inputs.shape[3]\n        channels = inputs.shape[4]\n        \n        states = [None] * len(self.layers_list)\n        outputs = []\n        for t in range(time_steps):\n            x = inputs[:, t]\n            for i, layer in enumerate(self.layers_list):\n                if states[i] is None:\n                    states[i] = tf.zeros((batch_size, height, width, layer.filters), dtype=tf.float32)\n                x, states[i] = layer(x, states[i])\n            outputs.append(x)\n        outputs = tf.stack(outputs, axis=1)\n        channels = self.layers_list[-1].filters\n        outputs = tf.reshape(outputs, (-1, height, width, channels))\n        outputs = self.conv_output(outputs)\n        outputs = tf.reshape(outputs, (batch_size, time_steps, height, width, -1))\n        return outputs\n\n# Model Initialization and Compilation\ninput_shape = (INPUT_LENGTH, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL)\noutput_channels = IMG_CHANNEL\nhidden_size = 64\nmodel = PredRNN(input_shape[1:], hidden_size, output_channels)\nmodel.build((None,) + input_shape)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n    loss=\"mse\",\n    metrics=[pixel_accuracy]  # Adding pixel accuracy metric\n)\n\n# Model Training\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=BATCH_SIZE, epochs=EPOCHS)\n\n# Save the model and weights\nSAVE_DIR = \"/kaggle/working/checkpoints/\"\nos.makedirs(SAVE_DIR, exist_ok=True)\nmodel.save(os.path.join(SAVE_DIR, \"predrnn_model.h5\"))\nmodel.save_weights(os.path.join(SAVE_DIR, \"predrnn_model_weights.weights.h5\"))\n\n# Test and Save Results\npredictions = model.predict(x_val[:5])\nRESULTS_DIR = \"/kaggle/working/results/\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nfor i, prediction in enumerate(predictions):\n    output_dir = os.path.join(RESULTS_DIR, f\"sample_{i}\")\n    os.makedirs(output_dir, exist_ok=True)\n    for t, frame in enumerate(prediction):\n        cv2.imwrite(os.path.join(output_dir, f\"frame_{t + 1}.png\"), (frame.squeeze() * 255).astype(np.uint8))\n\nprint(f\"Results saved in: {RESULTS_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:43:30.910031Z","iopub.status.idle":"2024-12-02T14:43:30.910361Z","shell.execute_reply.started":"2024-12-02T14:43:30.910212Z","shell.execute_reply":"2024-12-02T14:43:30.910229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import imageio\n\n# Function to create a GIF\ndef create_gif(frames, gif_path):\n    # Convert frames to uint8 and write to GIF\n    frames = [(frame * 255).astype(np.uint8) for frame in frames]\n    imageio.mimsave(gif_path, frames, fps=10)  # Save GIF at 10 FPS\n\n# Save results with ground truth comparison\nRESULTS_DIR = \"/kaggle/working/results/\"\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nfor i, (prediction, ground_truth) in enumerate(zip(predictions, y_val[:5])):\n    output_dir = os.path.join(RESULTS_DIR, f\"sample_{i}\")\n    os.makedirs(output_dir, exist_ok=True)\n    \n    prediction_frames = []\n    ground_truth_frames = []\n    combined_frames = []\n\n    for t in range(prediction.shape[0]):\n        pred_frame = (prediction[t].squeeze() * 255).astype(np.uint8)  # Convert prediction to uint8\n        gt_frame = (ground_truth[t].squeeze() * 255).astype(np.uint8)  # Convert ground truth to uint8\n\n        # Save individual frames for debugging\n        cv2.imwrite(os.path.join(output_dir, f\"pred_frame_{t + 1}.png\"), pred_frame)\n        cv2.imwrite(os.path.join(output_dir, f\"gt_frame_{t + 1}.png\"), gt_frame)\n\n        # Store frames for GIF creation\n        prediction_frames.append(pred_frame)\n        ground_truth_frames.append(gt_frame)\n\n        # Create side-by-side comparison\n        combined_frame = np.hstack((gt_frame, pred_frame))  # Horizontal stack\n        combined_frames.append(combined_frame)\n\n    # Save GIFs\n    create_gif(prediction_frames, os.path.join(output_dir, \"prediction.gif\"))\n    create_gif(ground_truth_frames, os.path.join(output_dir, \"ground_truth.gif\"))\n    create_gif(combined_frames, os.path.join(output_dir, \"comparison.gif\"))\n\nprint(f\"GIFs and frames saved in: {RESULTS_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:43:30.911654Z","iopub.status.idle":"2024-12-02T14:43:30.911962Z","shell.execute_reply.started":"2024-12-02T14:43:30.911820Z","shell.execute_reply":"2024-12-02T14:43:30.911836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.metrics import structural_similarity as ssim\nfrom sklearn.metrics import mean_squared_error\nimport cv2\nimport os\n\n# Function to compute average MSE and SSIM from saved prediction and ground truth frames\ndef compute_metrics_from_saved_frames(results_dir):\n    \"\"\"\n    Compute average MSE and SSIM from saved prediction and ground truth frames.\n    \"\"\"\n    total_mse = 0\n    total_ssim = 0\n    total_frames = 0\n\n    # Iterate through each sample directory\n    for sample_dir in os.listdir(results_dir):\n        sample_path = os.path.join(results_dir, sample_dir)\n        if not os.path.isdir(sample_path):\n            continue\n\n        # Load prediction and ground truth frames\n        pred_frames = sorted(\n            [os.path.join(sample_path, f) for f in os.listdir(sample_path) if f.startswith(\"pred_frame\")]\n        )\n        gt_frames = sorted(\n            [os.path.join(sample_path, f) for f in os.listdir(sample_path) if f.startswith(\"gt_frame\")]\n        )\n\n        for pred_file, gt_file in zip(pred_frames, gt_frames):\n            # Read frames\n            pred_frame = cv2.imread(pred_file, cv2.IMREAD_GRAYSCALE)\n            gt_frame = cv2.imread(gt_file, cv2.IMREAD_GRAYSCALE)\n\n            # Ensure frames are the same size\n            assert pred_frame.shape == gt_frame.shape, f\"Shape mismatch: {pred_file} and {gt_file}\"\n\n            # Compute MSE\n            mse_value = mean_squared_error(gt_frame.flatten(), pred_frame.flatten())\n            total_mse += mse_value\n\n            # Compute SSIM\n            ssim_value, _ = ssim(gt_frame, pred_frame, full=True)\n            total_ssim += ssim_value\n\n            total_frames += 1\n\n    # Calculate averages\n    avg_mse = total_mse / total_frames\n    avg_ssim = total_ssim / total_frames\n    return avg_mse, avg_ssim\n\n# Calculate metrics\nprint(\"Calculating MSE and SSIM...\")\navg_mse, avg_ssim = compute_metrics_from_saved_frames(RESULTS_DIR)\n\nprint(f\"Average MSE: {avg_mse:.4f}\")\nprint(f\"Average SSIM: {avg_ssim:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:43:30.913138Z","iopub.status.idle":"2024-12-02T14:43:30.913410Z","shell.execute_reply.started":"2024-12-02T14:43:30.913274Z","shell.execute_reply":"2024-12-02T14:43:30.913289Z"}},"outputs":[],"execution_count":null}]}