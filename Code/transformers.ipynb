{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4849320,"sourceType":"datasetVersion","datasetId":2807884}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport kagglehub\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchmetrics import StructuralSimilarityIndexMeasure as SSIM\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom torchvision import transforms\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:47:07.602524Z","iopub.execute_input":"2024-12-02T14:47:07.603524Z","iopub.status.idle":"2024-12-02T14:47:14.937683Z","shell.execute_reply.started":"2024-12-02T14:47:07.603469Z","shell.execute_reply":"2024-12-02T14:47:14.936710Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n\ndef print_counts(path):\n    # Initialize a dictionary to store folder names and their .avi file counts\n    folder_avi_counts = {}\n\n    # Loop through each folder in the root directory\n    for folder_name in os.listdir(path):\n        folder_path = os.path.join(path, folder_name)\n        if os.path.isdir(folder_path):\n            # Count the .avi files in the current folder\n            avi_count = len([file for file in os.listdir(folder_path) if file.endswith('.avi')])\n            folder_avi_counts[folder_name] = avi_count\n\n    # Sort the folder_avi_counts dictionary by folder name for a cleaner chart\n    sorted_counts = dict(sorted(folder_avi_counts.items()))\n\n    # Plot the vertical bar chart for all folder counts\n    plt.figure(figsize=(12, 6))\n    plt.bar(sorted_counts.keys(), sorted_counts.values(), color='skyblue')\n    plt.xlabel('Folder Names', fontsize=12)\n    plt.ylabel('Count of .avi Files', fontsize=12)\n    plt.title('Count of .avi Files in Each Folder', fontsize=14)\n    plt.xticks(rotation=90, fontsize=10)  # Rotate x-axis labels for readability\n    plt.tight_layout()  # Adjust layout to fit labels\n    plt.show()\n\n    # Get the top 5 folders with the highest .avi counts\n    top_5_counts = dict(sorted(folder_avi_counts.items(), key=lambda x: x[1], reverse=True)[:5])\n\n    # Plot the horizontal bar chart for top 5 folders\n    plt.figure(figsize=(10, 6))\n    plt.barh(list(top_5_counts.keys()), list(top_5_counts.values()), color='coral')\n    plt.xlabel('Count of .avi Files', fontsize=12)\n    plt.ylabel('Folder Names', fontsize=12)\n    plt.title('Top 5 Folders with Highest Count of .avi Files', fontsize=14)\n    plt.gca().invert_yaxis()  # Invert y-axis to display the highest count at the top\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:47:14.939161Z","iopub.execute_input":"2024-12-02T14:47:14.939564Z","iopub.status.idle":"2024-12-02T14:47:14.947496Z","shell.execute_reply.started":"2024-12-02T14:47:14.939538Z","shell.execute_reply":"2024-12-02T14:47:14.946597Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_path = \"/kaggle/input/ucf101-action-recognition/train\"\nval_path = \"/kaggle/input/ucf101-action-recognition/val\"\ntest_path = \"/kaggle/input/ucf101-action-recognition/test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:47:14.948558Z","iopub.execute_input":"2024-12-02T14:47:14.948795Z","iopub.status.idle":"2024-12-02T14:47:14.962373Z","shell.execute_reply.started":"2024-12-02T14:47:14.948773Z","shell.execute_reply":"2024-12-02T14:47:14.961693Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"selected_classes = [\"HorseRiding\", \"PlayingDhol\", \"PushUps\", \"BenchPress\", \"PlayingGuitar\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:47:14.964713Z","iopub.execute_input":"2024-12-02T14:47:14.965299Z","iopub.status.idle":"2024-12-02T14:47:14.974325Z","shell.execute_reply.started":"2024-12-02T14:47:14.965270Z","shell.execute_reply":"2024-12-02T14:47:14.973523Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"output_path = \"/kaggle/working/processed_ucf101_train\"  # Path to save processed data\n\n# Parameters for preprocessing\nframe_size = (64, 64)  # Target size for frames\nconvert_to_grayscale = False  # Set to True to convert to grayscale; False for RGB\n\n# Function to preprocess videos\ndef preprocess_videos(output_path, file_path):\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n\n    for class_name in selected_classes:\n        class_path = os.path.join(file_path, class_name)\n        processed_class_path = os.path.join(output_path, class_name)\n\n        if not os.path.exists(processed_class_path):\n            os.makedirs(processed_class_path)\n\n        # Loop through each video in the class folder\n        for video_file in os.listdir(class_path):\n            if video_file.endswith(\".avi\"):\n                video_path = os.path.join(class_path, video_file)\n                video_capture = cv2.VideoCapture(video_path)\n\n                frames = []\n                while True:\n                    ret, frame = video_capture.read()\n                    if not ret:\n                        break\n\n                    # Resize frame\n                    frame = cv2.resize(frame, frame_size)\n\n                    # Convert to grayscale if needed\n                    if convert_to_grayscale:\n                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n                    frames.append(frame)\n\n                video_capture.release()\n\n                # Save preprocessed frames as a numpy array\n                frames = np.array(frames)  # Shape: (num_frames, 64, 64, 3) for RGB or (num_frames, 64, 64) for grayscale\n                save_path = os.path.join(processed_class_path, video_file.replace(\".avi\", \".npy\"))\n                np.save(save_path, frames)\n\n                #print(f\"Processed and saved: {save_path}\")\n\n# Run preprocessing\npreprocess_videos(output_path, train_path)\nval_output_path = \"/kaggle/working/processed_ucf101_val\"\npreprocess_videos(val_output_path, val_path)\ntest_output_path = \"/kaggle/working/processed_ucf101_test\"\npreprocess_videos(test_output_path, test_path)\nprint(\"Processed and saved all videos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:47:14.975542Z","iopub.execute_input":"2024-12-02T14:47:14.975877Z","iopub.status.idle":"2024-12-02T14:47:48.147026Z","shell.execute_reply.started":"2024-12-02T14:47:14.975841Z","shell.execute_reply":"2024-12-02T14:47:48.146045Z"}},"outputs":[{"name":"stdout","text":"Processed and saved all videos\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras import layers, Model\nimport os\n\n# Custom pixel-level accuracy metric\ndef pixel_accuracy(y_true, y_pred, threshold=0.5):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    correct_pixels = tf.abs(y_true - y_pred) < threshold\n    accuracy = tf.reduce_mean(tf.cast(correct_pixels, tf.float32))\n    return accuracy\n\n# Dataset Class with Noise Augmentation\nclass VideoFrameDataset(tf.keras.utils.Sequence):\n    def __init__(self, data_path, input_frames=5, future_frames=5, batch_size=8):\n        self.data_path = data_path\n        self.input_frames = input_frames\n        self.future_frames = future_frames\n        self.batch_size = batch_size\n        self.video_files = []\n\n        for class_name in os.listdir(data_path):\n            class_path = os.path.join(data_path, class_name)\n            if os.path.isdir(class_path):\n                for video_file in os.listdir(class_path):\n                    if video_file.endswith('.npy'):\n                        self.video_files.append(os.path.join(class_path, video_file))\n\n    def __len__(self):\n        return len(self.video_files) // self.batch_size\n\n    def __getitem__(self, idx):\n        batch_files = self.video_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        X, y = [], []\n\n        for file_path in batch_files:\n            video = np.load(file_path)  # Video shape: (frames, H, W, 3)\n            if len(video) >= (self.input_frames + self.future_frames):\n                # Convert video to grayscale\n                video = np.mean(video, axis=-1, keepdims=True)  # Grayscale: (frames, H, W, 1)\n\n                start_idx = np.random.randint(0, len(video) - (self.input_frames + self.future_frames) + 1)\n                input_seq = video[start_idx:start_idx + self.input_frames]\n                target_seq = video[start_idx + self.input_frames:start_idx + self.input_frames + self.future_frames]\n\n                # Normalize and add noise\n                input_seq = self.add_noise(input_seq.astype(np.float32) / 255.0)\n                target_seq = target_seq.astype(np.float32) / 255.0\n\n                X.append(input_seq)\n                y.append(target_seq)\n\n        return np.array(X), np.array(y)\n\n    @staticmethod\n    def add_noise(video, noise_factor=0.05):\n        noisy_video = video + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=video.shape)\n        return np.clip(noisy_video, 0.0, 1.0)\n\n\n# Patch Embedding Layer\nclass PatchEmbedding(layers.Layer):\n    def __init__(self, patch_size, embed_dim):\n        super().__init__()\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        self.projection = layers.Conv3D(\n            filters=embed_dim,\n            kernel_size=(1, patch_size, patch_size),\n            strides=(1, patch_size, patch_size),\n            padding=\"valid\"\n        )\n\n    def call(self, inputs):\n        x = self.projection(inputs)\n        shape = tf.shape(x)\n        batch_size, frames, h, w = shape[0], shape[1], shape[2], shape[3]\n        x = tf.reshape(x, [batch_size, frames, h * w, self.embed_dim])\n        return x\n\n\n# Transformer Block with Residual Connections\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            layers.Dense(ff_dim, activation=\"gelu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(dropout)\n        self.dropout2 = layers.Dropout(dropout)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return inputs + self.layernorm2(out1 + ffn_output)  # Double residual connection\n\n\n# Reconstruction Layer\nclass FrameReconstruction(layers.Layer):\n    def __init__(self, filters=64, embed_dim=256, patch_size=4):\n        super().__init__()\n        self.conv1 = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")\n        self.conv2 = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")\n        self.conv3 = layers.Conv2D(embed_dim, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")\n        self.conv_transpose = layers.Conv2DTranspose(\n            filters=1,\n            kernel_size=patch_size,\n            strides=patch_size,\n            padding=\"same\",\n            activation=\"sigmoid\"\n        )\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv_transpose(x)\n        return x\n\n\n# ViViT Model\nclass ViViT(Model):\n    def __init__(\n        self, input_shape, patch_size=4, embed_dim=256, num_heads=12,\n        ff_dim=512, num_transformer_layers=10, dropout=0.1, future_frames=5\n    ):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n        frames, h, w, c = input_shape\n        self.num_patches = (h // patch_size) * (w // patch_size)\n        self.embed_dim = embed_dim\n\n        # Positional embedding updated to include num_patches\n        self.pos_embed = self.add_weight(\n            name=\"pos_embed\",\n            shape=[1, frames, self.num_patches, embed_dim],\n            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n            trainable=True\n        )\n\n        self.transformer_blocks = [\n            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)\n            for _ in range(num_transformer_layers)\n        ]\n        self.future_frames = future_frames\n        self.reconstruction = FrameReconstruction(embed_dim=embed_dim, patch_size=patch_size)\n\n    def call(self, inputs, training=False):\n        x = self.patch_embed(inputs)\n        # Add positional embedding, matching all dimensions\n        x += self.pos_embed[:, :tf.shape(x)[1], :, :]\n\n        for block in self.transformer_blocks:\n            x = block(x, training=training)\n\n        future_frames = []\n        current_embedding = x[:, -1]\n        patch_dim = tf.cast(tf.sqrt(tf.cast(self.num_patches, tf.float32)), tf.int32)\n\n        for _ in range(self.future_frames):\n            current_embedding_reshaped = tf.reshape(\n                current_embedding,\n                [-1, patch_dim, patch_dim, self.embed_dim]\n            )\n            reconstructed_frame = self.reconstruction(current_embedding_reshaped)\n            future_frames.append(reconstructed_frame)\n\n            reconstructed_frame_expanded = tf.expand_dims(reconstructed_frame, axis=1)\n            reconstructed_frame_patches = self.patch_embed(reconstructed_frame_expanded)\n            current_embedding = tf.reduce_mean(reconstructed_frame_patches, axis=1)\n\n        return tf.stack(future_frames, axis=1)\n\n\n# Loss Functions\ndef ssim_loss(y_true, y_pred):\n    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n\ndef combined_loss(y_true, y_pred):\n    mse = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n    ssim = ssim_loss(y_true, y_pred)\n    return 0.7 * mse + 0.3 * ssim\n\n\n# Training Function\ndef train_model(model, train_dataset, val_dataset, epochs=50):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(optimizer=optimizer, loss=combined_loss, metrics=[\"mse\", ssim_loss, pixel_accuracy])  # Added pixel_accuracy metric\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=epochs,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n            tf.keras.callbacks.ModelCheckpoint(\"vivit_grayscale_sharp_model.keras\", save_best_only=True)\n        ]\n    )\n    return history\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:47:48.148245Z","iopub.execute_input":"2024-12-02T14:47:48.148538Z","iopub.status.idle":"2024-12-02T14:47:58.693770Z","shell.execute_reply.started":"2024-12-02T14:47:48.148511Z","shell.execute_reply":"2024-12-02T14:47:58.692811Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Specify paths for training and validation datasets\ntrain_path = \"/kaggle/working/processed_ucf101_train\"\nval_path = \"/kaggle/working/processed_ucf101_val\"\n\n# Create datasets\ntrain_dataset = VideoFrameDataset(train_path, input_frames=10, future_frames=10, batch_size=4)\nval_dataset = VideoFrameDataset(val_path, input_frames=10, future_frames=10, batch_size=4)\n\n \n# Modify the ViViT class to accept this shape\nmodel = ViViT(\n    input_shape=( 10, 64, 64, 1),  # 5 frames, 64x64, 1 channel (grayscale)\n    patch_size=8,                # Patch size\n    embed_dim=256,               # Embedding dimension\n    num_heads=16,                 # Number of attention heads\n    ff_dim=512,                  # Feedforward dimension\n    num_transformer_layers=16,    # Number of transformer layers\n    dropout=0.1,                 # Dropout rate\n    future_frames=10             # Predict 5 future frames\n)\n\n# Train the model\nhistory = train_model(model, train_dataset, val_dataset, epochs=60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T14:47:58.695077Z","iopub.execute_input":"2024-12-02T14:47:58.695793Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/60\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733150923.129570    3096 service.cc:145] XLA service 0x7b4918022c90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1733150923.129628    3096 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nW0000 00:00:1733150927.551258    3096 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert/Assert\nW0000 00:00:1733150927.552496    3096 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert_1/Assert\nW0000 00:00:1733150927.553970    3096 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert_2/Assert\nW0000 00:00:1733150927.555043    3096 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert_3/Assert\nW0000 00:00:1733150927.555245    3096 assert_op.cc:38] Ignoring Assert operator SSIM/Assert/Assert\nW0000 00:00:1733150927.555428    3096 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_1/Assert\nW0000 00:00:1733150927.556571    3096 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_2/Assert\nW0000 00:00:1733150927.557128    3096 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_3/Assert\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733151024.555247    3096 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_175', 224 bytes spill stores, 224 bytes spill loads\n\nI0000 00:00:1733151024.666098    3096 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step - loss: 0.3135 - mse: 0.0886 - pixel_accuracy: 0.9839 - ssim_loss: 0.8383","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1733151090.397270    3098 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert/Assert\nW0000 00:00:1733151090.398462    3098 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert_1/Assert\nW0000 00:00:1733151090.399481    3098 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert_2/Assert\nW0000 00:00:1733151090.400118    3098 assert_op.cc:38] Ignoring Assert operator compile_loss/combined_loss/SSIM/Assert_3/Assert\nW0000 00:00:1733151090.400216    3098 assert_op.cc:38] Ignoring Assert operator SSIM/Assert/Assert\nW0000 00:00:1733151090.400373    3098 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_1/Assert\nW0000 00:00:1733151090.401431    3098 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_2/Assert\nW0000 00:00:1733151090.402118    3098 assert_op.cc:38] Ignoring Assert operator SSIM/Assert_3/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 536ms/step - loss: 0.3134 - mse: 0.0886 - pixel_accuracy: 0.9839 - ssim_loss: 0.8382 - val_loss: 0.3028 - val_mse: 0.0777 - val_pixel_accuracy: 0.9632 - val_ssim_loss: 0.8281\nEpoch 2/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 490ms/step - loss: 0.2964 - mse: 0.0736 - pixel_accuracy: 0.9614 - ssim_loss: 0.8163 - val_loss: 0.2977 - val_mse: 0.0742 - val_pixel_accuracy: 0.9526 - val_ssim_loss: 0.8193\nEpoch 3/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 491ms/step - loss: 0.2915 - mse: 0.0701 - pixel_accuracy: 0.9566 - ssim_loss: 0.8083 - val_loss: 0.2970 - val_mse: 0.0724 - val_pixel_accuracy: 0.9509 - val_ssim_loss: 0.8212\nEpoch 4/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 490ms/step - loss: 0.2918 - mse: 0.0703 - pixel_accuracy: 0.9529 - ssim_loss: 0.8085 - val_loss: 0.2958 - val_mse: 0.0718 - val_pixel_accuracy: 0.9507 - val_ssim_loss: 0.8186\nEpoch 5/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 490ms/step - loss: 0.2920 - mse: 0.0713 - pixel_accuracy: 0.9543 - ssim_loss: 0.8069 - val_loss: 0.2907 - val_mse: 0.0698 - val_pixel_accuracy: 0.9569 - val_ssim_loss: 0.8060\nEpoch 6/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 491ms/step - loss: 0.2843 - mse: 0.0663 - pixel_accuracy: 0.9618 - ssim_loss: 0.7927 - val_loss: 0.2828 - val_mse: 0.0660 - val_pixel_accuracy: 0.9610 - val_ssim_loss: 0.7887\nEpoch 7/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 489ms/step - loss: 0.2745 - mse: 0.0623 - pixel_accuracy: 0.9686 - ssim_loss: 0.7696 - val_loss: 0.2720 - val_mse: 0.0606 - val_pixel_accuracy: 0.9708 - val_ssim_loss: 0.7655\nEpoch 8/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 490ms/step - loss: 0.2664 - mse: 0.0581 - pixel_accuracy: 0.9747 - ssim_loss: 0.7524 - val_loss: 0.2669 - val_mse: 0.0574 - val_pixel_accuracy: 0.9693 - val_ssim_loss: 0.7556\nEpoch 9/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 488ms/step - loss: 0.2612 - mse: 0.0540 - pixel_accuracy: 0.9752 - ssim_loss: 0.7447 - val_loss: 0.2611 - val_mse: 0.0539 - val_pixel_accuracy: 0.9716 - val_ssim_loss: 0.7444\nEpoch 10/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 489ms/step - loss: 0.2548 - mse: 0.0515 - pixel_accuracy: 0.9766 - ssim_loss: 0.7292 - val_loss: 0.2602 - val_mse: 0.0563 - val_pixel_accuracy: 0.9611 - val_ssim_loss: 0.7358\nEpoch 11/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 490ms/step - loss: 0.2548 - mse: 0.0500 - pixel_accuracy: 0.9769 - ssim_loss: 0.7326 - val_loss: 0.2572 - val_mse: 0.0528 - val_pixel_accuracy: 0.9708 - val_ssim_loss: 0.7339\nEpoch 12/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 490ms/step - loss: 0.2505 - mse: 0.0477 - pixel_accuracy: 0.9773 - ssim_loss: 0.7236 - val_loss: 0.2492 - val_mse: 0.0474 - val_pixel_accuracy: 0.9743 - val_ssim_loss: 0.7201\nEpoch 13/60\n\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 491ms/step - loss: 0.2474 - mse: 0.0457 - pixel_accuracy: 0.9776 - ssim_loss: 0.7180 - val_loss: 0.2406 - val_mse: 0.0419 - val_pixel_accuracy: 0.9840 - val_ssim_loss: 0.7040\nEpoch 14/60\n\u001b[1m 62/140\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 439ms/step - loss: 0.2367 - mse: 0.0417 - pixel_accuracy: 0.9823 - ssim_loss: 0.6915","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model_save_path = \"/kaggle/working/transformers_model.h5\"\nmodel.save(model_save_path)\nprint(f\"Model saved successfully at {model_save_path}\")\nmodel_weights_save_path = \"/kaggle/working/transformers_model.weights.h5\"\nprint(f\"Model weights saved successfully at {model_weights_save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"\n    Plots the training and validation loss, MSE, and SSIM metrics.\n    Args:\n    - history: The history object returned by `model.fit`.\n    \"\"\"\n    # Plot training & validation loss\n    plt.figure(figsize=(12, 6))\n\n    # Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Val Loss')\n    plt.title('Loss (MSE + SSIM)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # MSE\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['mse'], label='Train MSE')\n    plt.plot(history.history['val_mse'], label='Val MSE')\n    plt.title('Mean Squared Error (MSE)')\n    plt.xlabel('Epoch')\n    plt.ylabel('MSE')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # # Optionally plot SSIM (if included in metrics)\n    # plt.figure(figsize=(6, 6))\n    # plt.plot(history.history['lambda'], label='Train SSIM')\n    # plt.plot(history.history['val_lambda'], label='Val SSIM')\n    # plt.title('SSIM (Structural Similarity Index)')\n    # plt.xlabel('Epoch')\n    # plt.ylabel('SSIM')\n    # plt.legend()\n    # plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_future_frames(model, input_sequence):\n    \"\"\"\n    Predict future frames using the trained model.\n    Args:\n        model: Trained ViViT model.\n        input_sequence: Input frames of shape (batch_size, input_frames, height, width, channels).\n    Returns:\n        Predicted future frames of shape (batch_size, future_frames, height, width, channels).\n    \"\"\"\n    input_sequence = np.expand_dims(input_sequence, axis=0)  # Add batch dimension if single input\n    input_sequence = input_sequence.astype(np.float32) / 255.0  # Normalize to [0, 1]\n    \n    predicted_frames = model.predict(input_sequence)  # Predict future frames\n    predicted_frames = np.clip(predicted_frames, 0, 1)  # Ensure values are in valid range\n    predicted_frames = (predicted_frames * 255).astype(np.uint8)  # Convert back to [0, 255]\n    \n    return predicted_frames\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\n\ndef save_frames_to_directory(frames, output_directory):\n    \"\"\"\n    Save frames as individual image files in the specified directory.\n    Args:\n        frames: Array of frames with shape (num_frames, height, width, channels).\n        output_directory: Path to save the frames.\n    \"\"\"\n    os.makedirs(output_directory, exist_ok=True)  # Create the directory if it doesn't exist\n    \n    for i, frame in enumerate(frames):\n        frame_path = os.path.join(output_directory, f\"frame_{i:04d}.png\")  # Save as frame_0001.png, frame_0002.png, etc.\n        Image.fromarray(frame).save(frame_path)\n\n# Example usage\n#frames_to_save = predicted_frames[0]  # Assuming predicted_frames[0] contains the frames\n#frames_to_save = [np.squeeze(frame) for frame in frames_to_save]  # Adjust if grayscale\n\n# Save frames in kaggle/working/predictions\n#save_frames_to_directory(frames_to_save, \"/kaggle/working/predictions\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_predictions(input_frames, predicted_frames):\n    \"\"\"\n    Visualize the input and predicted future frames.\n    Args:\n        input_frames: Array of input frames of shape (input_frames, height, width, channels).\n        predicted_frames: Array of predicted frames of shape (future_frames, height, width, channels).\n    \"\"\"\n    num_input_frames = input_frames.shape[0]\n    num_predicted_frames = predicted_frames.shape[0]\n\n    fig, axes = plt.subplots(2, max(num_input_frames, num_predicted_frames), figsize=(15, 5))\n    \n    # Plot input frames\n    for i in range(num_input_frames):\n        axes[0, i].imshow(input_frames[i, :, :, 0], cmap=\"gray\")\n        axes[0, i].axis(\"off\")\n        axes[0, i].set_title(f\"Input Frame {i+1}\")\n\n    # Plot predicted frames\n    for i in range(num_predicted_frames):\n        axes[1, i].imshow(predicted_frames[i, :, :, 0], cmap=\"gray\")\n        axes[1, i].axis(\"off\")\n        axes[1, i].set_title(f\"Predicted Frame {i+1}\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example input video sequence (you can use any sequence from your dataset)\ntest_video_path = \"/kaggle/working/processed_ucf101_test/BenchPress/v_BenchPress_g01_c02.npy\"\ntest_video = np.load(test_video_path)  # Shape: (frames, height, width, 3)\n\n# Convert to grayscale\ntest_video = np.mean(test_video, axis=-1, keepdims=True)  # Shape: (frames, height, width, 1)\n\n# Select input frames\ninput_frames = test_video[:10]  # First 5 frames as input\n\n# Predict future frames\npredicted_frames = predict_future_frames(model, input_frames)\npredicted_frames_squeezed = np.squeeze(predicted_frames, axis=0)  # Shape: (10, 64, 64, 1)\n\nprint(\"Shape of the array:\", predicted_frames.shape)\n\nframes_to_save = predicted_frames[0]  # Assuming predicted_frames[0] contains the frames\nframes_to_save = [np.squeeze(frame) for frame in frames_to_save]  # Adjust if grayscale\n\n# Save frames in kaggle/working/predictions\nsave_frames_to_directory(frames_to_save, \"/kaggle/working/predictions\")\n\ndef create_gif_from_directory(input_directory, output_gif_path, duration=500):\n    \"\"\"\n    Create a GIF from image frames stored in a directory.\n    Args:\n        input_directory: Path to the directory containing image frames.\n        output_gif_path: Path to save the generated GIF.\n        duration: Duration of each frame in milliseconds.\n    \"\"\"\n    # Get all image file paths sorted by name\n    frame_files = sorted(\n        [os.path.join(input_directory, f) for f in os.listdir(input_directory) if f.endswith('.png')]\n    )\n    \n    # Load images\n    frames = [Image.open(frame_file) for frame_file in frame_files]\n    \n    # Create and save the GIF\n    frames[0].save(\n        output_gif_path,\n        save_all=True,\n        append_images=frames[1:],\n        duration=duration,\n        loop=0\n    )\n\ninput_directory = \"/kaggle/working/predictions\"\noutput_gif_path = \"/kaggle/working/predicted_frames.gif\"\ncreate_gif_from_directory(input_directory, output_gif_path, duration=500)\n\n\n\n\n# Visualize results\nvisualize_predictions(input_frames, predicted_frames_squeezed)  # Visualize first predicted batch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example input video sequence (you can use any sequence from your dataset)\ntest_video_path = \"/kaggle/working/processed_ucf101_test/PushUps/v_PushUps_g09_c03.npy\"\ntest_video = np.load(test_video_path)  # Shape: (frames, height, width, 3)\n\n# Convert to grayscale\ntest_video = np.mean(test_video, axis=-1, keepdims=True)  # Shape: (frames, height, width, 1)\n\n# Select input frames\ninput_frames = test_video[:10]  # First 10 frames as input\n\n# Predict future frames\npredicted_frames = predict_future_frames(model, input_frames)\npredicted_frames_squeezed = np.squeeze(predicted_frames, axis=0)  # Shape: (10, 64, 64, 1)\n\n# Visualize results\nvisualize_predictions(input_frames, predicted_frames_squeezed)  # Visualize first predicted batch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_weights_save_path = \"/kaggle/working/transformers_model.weights.h5\"\nmodel.save_weights(model_weights_save_path)\n\nprint(f\"Model weights saved successfully at {model_weights_save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Provide the path to your file\nfile_path = '/kaggle/working/transformers_model.weights.h5'\n\n# Create a downloadable link\nFileLink(file_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}